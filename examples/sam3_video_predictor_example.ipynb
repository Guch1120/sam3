{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11912666",
   "metadata": {},
   "source": [
    "# SAM 3 ビデオセグメンテーションと追跡 (Video segmentation and tracking with SAM 3)\n",
    "\n",
    "このノートブックでは、SAM 3を使用してインタラクティブなビデオセグメンテーションと高密度追跡（dense tracking）を行う方法を解説します。\n",
    "以下の機能について学びます：\n",
    "\n",
    "- **テキストプロンプト**: 自然言語（例：「person」、「shoe」）を使用してオブジェクトをセグメントします。\n",
    "- **ポイントプロンプト**: ポジティブ/ネガティブクリックを追加して、オブジェクトをセグメントおよび修正します。\n",
    "\n",
    "用語について：\n",
    "- **セグメント (segment)** または **マスク (mask)**: 単一フレーム上のオブジェクトに対するモデルの予測結果を指します。\n",
    "- **マスクレット (masklet)**: ビデオ全体にわたる時空間的なマスクを指します。\n",
    "\n",
    "# <a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam3/blob/main/notebooks/sam3_video_predictor_example.ipynb\">\n",
    "#   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "# </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-setup-md",
   "metadata": {},
   "source": [
    "## Google Colab セットアップ\n",
    "\n",
    "Google Colabで実行する場合は、以下のセルを実行してください。\n",
    "これにより、必要なライブラリのインストールと、データの読み込み準備が行われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境のセットアップ\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Google Colab環境で実行中\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ローカル環境で実行中\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # 1. Google Driveをマウント (推奨)\n",
    "    # 自分のデータやコードを使用する場合は、Google Driveにアップロードしてマウントします\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # 2. SAM3のインストールとパス設定\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    # Google Drive内のSAM3ディレクトリのパス\n",
    "    # ※ご自身の環境に合わせてパスを変更してください\n",
    "    DRIVE_SAM3_PATH = \"/content/drive/MyDrive/sam3\"\n",
    "    \n",
    "    if os.path.exists(DRIVE_SAM3_PATH):\n",
    "        print(f\"Google Drive内のSAM3が見つかりました: {DRIVE_SAM3_PATH}\")\n",
    "        os.chdir(DRIVE_SAM3_PATH)\n",
    "        print(\"カレントディレクトリを変更しました。\")\n",
    "        \n",
    "        # 依存関係のインストール\n",
    "        print(\"依存関係をインストールしています...\")\n",
    "        # Numpy 2.0との互換性問題を回避するためにバージョンを固定\n",
    "        !pip install -q \"numpy<2.0\"\n",
    "        !pip install -q -e .\n",
    "        !pip install -q pycocotools\n",
    "        \n",
    "    else:\n",
    "        print(f\"Google Drive内に {DRIVE_SAM3_PATH} が見つかりませんでした。\")\n",
    "        print(\"GitHubからSAM3をクローンしてインストールします...\")\n",
    "        \n",
    "        # GitHubからクローン\n",
    "        if not os.path.exists(\"/content/sam3\"):\n",
    "            !git clone https://github.com/facebookresearch/sam3.git /content/sam3\n",
    "            \n",
    "        os.chdir(\"/content/sam3\")\n",
    "        print(\"カレントディレクトリを /content/sam3 に変更しました。\")\n",
    "        \n",
    "        # 依存関係のインストール\n",
    "        # Numpy 2.0との互換性問題を回避するためにバージョンを固定\n",
    "        !pip install -q \"numpy<2.0\"\n",
    "        !pip install -q -e .\n",
    "        !pip install -q pycocotools\n",
    "    \n",
    "    print(\"セットアップが完了しました。\")\n",
    "    print(f\"現在の作業ディレクトリ: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-data-usage",
   "metadata": {},
   "source": [
    "### データの使用について\n",
    "\n",
    "- **Google Driveを使用する場合**: `GT_DIR` や `PRED_DIR` には `/content/drive/MyDrive/...` から始まるパスを指定してください。\n",
    "- **GitHubからクローンした場合**: 左側のファイルブラウザから `/content/sam3` 内を確認できます。データは別途アップロードが必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境のセットアップ\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Google Colab環境で実行中\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ローカル環境で実行中\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # 1. Google Driveをマウント (推奨)\n",
    "    # 自分のデータやコードを使用する場合は、Google Driveにアップロードしてマウントします\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # 2. SAM3のインストールとパス設定\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    # Google Drive内のSAM3ディレクトリのパス\n",
    "    # ※ご自身の環境に合わせてパスを変更してください\n",
    "    DRIVE_SAM3_PATH = \"/content/drive/MyDrive/sam3\"\n",
    "    \n",
    "    if os.path.exists(DRIVE_SAM3_PATH):\n",
    "        print(f\"Google Drive内のSAM3が見つかりました: {DRIVE_SAM3_PATH}\")\n",
    "        os.chdir(DRIVE_SAM3_PATH)\n",
    "        print(\"カレントディレクトリを変更しました。\")\n",
    "        \n",
    "        # 依存関係のインストール\n",
    "        print(\"依存関係をインストールしています...\")\n",
    "        !pip install -q -e .\n",
    "        !pip install -q pycocotools\n",
    "        \n",
    "    else:\n",
    "        print(f\"Google Drive内に {DRIVE_SAM3_PATH} が見つかりませんでした。\")\n",
    "        print(\"GitHubからSAM3をクローンしてインストールします...\")\n",
    "        \n",
    "        # GitHubからクローン\n",
    "        if not os.path.exists(\"/content/sam3\"):\n",
    "            !git clone https://github.com/facebookresearch/sam3.git /content/sam3\n",
    "            \n",
    "        os.chdir(\"/content/sam3\")\n",
    "        print(\"カレントディレクトリを /content/sam3 に変更しました。\")\n",
    "        \n",
    "        # 依存関係のインストール\n",
    "        !pip install -q -e .\n",
    "        !pip install -q pycocotools\n",
    "    \n",
    "    print(\"セットアップが完了しました。\")\n",
    "    print(f\"現在の作業ディレクトリ: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-data-usage",
   "metadata": {},
   "source": [
    "### データの使用について\n",
    "\n",
    "- **Google Driveを使用する場合**: `GT_DIR` や `PRED_DIR` には `/content/drive/MyDrive/...` から始まるパスを指定してください。\n",
    "- **GitHubからクローンした場合**: 左側のファイルブラウザから `/content/sam3` 内を確認できます。データは別途アップロードが必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib scikit-learn\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam3.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-title",
   "metadata": {},
   "source": [
    "## セットアップ (Set-up)\n",
    "\n",
    "この例では、シングルGPUまたはマルチGPUでの推論が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sam3\n",
    "import torch\n",
    "\n",
    "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")\n",
    "\n",
    "# マシン上の利用可能なすべてのGPUを使用\n",
    "gpus_to_use = range(torch.cuda.device_count())\n",
    "# # シングルGPUのみを使用する場合\n",
    "# gpus_to_use = [torch.cuda.current_device()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-predictor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.model_builder import build_sam3_video_predictor\n",
    "\n",
    "predictor = build_sam3_video_predictor(gpus_to_use=gpus_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils-title",
   "metadata": {},
   "source": [
    "#### 推論と可視化のユーティリティ (Inference and visualization utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sam3.visualization_utils import (\n",
    "    load_frame,\n",
    "    prepare_masks_for_visualization,\n",
    "    visualize_formatted_frame_output,\n",
    ")\n",
    "\n",
    "# 軸タイトルのフォントサイズ\n",
    "plt.rcParams[\"axes.titlesize\"] = 12\n",
    "plt.rcParams[\"figure.titlesize\"] = 12\n",
    "\n",
    "\n",
    "def propagate_in_video(predictor, session_id):\n",
    "    # フレーム0からビデオの終わりまで伝播します\n",
    "    outputs_per_frame = {}\n",
    "    for response in predictor.handle_stream_request(\n",
    "        request=dict(\n",
    "            type=\"propagate_in_video\",\n",
    "            session_id=session_id,\n",
    "        )\n",
    "    ):\n",
    "        outputs_per_frame[response[\"frame_index\"]] = response[\"outputs\"]\n",
    "\n",
    "    return outputs_per_frame\n",
    "\n",
    "\n",
    "def abs_to_rel_coords(coords, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"):\n",
    "    \"\"\"絶対座標を相対座標（0-1の範囲）に変換します\n",
    "\n",
    "    Args:\n",
    "        coords: 座標のリスト\n",
    "        coord_type: [x, y] の場合は 'point'、[x, y, w, h] の場合は 'box'\n",
    "    \"\"\"\n",
    "    if coord_type == \"point\":\n",
    "        return [[x / IMG_WIDTH, y / IMG_HEIGHT] for x, y in coords]\n",
    "    elif coord_type == \"box\":\n",
    "        return [\n",
    "            [x / IMG_WIDTH, y / IMG_HEIGHT, w / IMG_WIDTH, h / IMG_HEIGHT]\n",
    "            for x, y, w, h in coords\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown coord_type: {coord_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-video-title",
   "metadata": {},
   "source": [
    "### 動画の読み込み (Loading an example video)\n",
    "\n",
    "動画は、**`<frame_index>.jpg` というファイル名のJPEGフレームのリスト**、または **MP4ビデオ** として保存されていることを想定しています。\n",
    "\n",
    "ffmpeg (https://ffmpeg.org/) を使用してJPEGフレームを抽出するには、以下のようにします：\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "ここで、`-q:v` は高品質のJPEGフレームを生成し、`-start_number 0` はffmpegに `00000.jpg` からJPEGファイルを開始するように指示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "set-video-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"video_path\" はJPEGフォルダまたはMP4ビデオファイルである必要があります\n",
    "video_path = f\"{sam3_root}/assets/videos/0001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-frames",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化のために \"video_frames_for_vis\" を読み込みます（モデルでは使用されません）\n",
    "if isinstance(video_path, str) and video_path.endswith(\".mp4\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_frames_for_vis = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames_for_vis.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "else:\n",
    "    video_frames_for_vis = glob.glob(os.path.join(video_path, \"*.jpg\"))\n",
    "    try:\n",
    "        # 文字列ソートではなく整数ソートを行います（例：\"2.jpg\" が \"11.jpg\" の前に来るように）\n",
    "        video_frames_for_vis.sort(\n",
    "            key=lambda p: int(os.path.splitext(os.path.basename(p))[0])\n",
    "        )\n",
    "    except ValueError:\n",
    "        # 形式が \"<frame_index>.jpg\" でない場合は辞書順ソートにフォールバックします\n",
    "        print(\n",
    "            f'frame names are not in \"<frame_index>.jpg\" format: {video_frames_for_vis[:5]=}, '\n",
    "            f\"falling back to lexicographic sort.\"\n",
    "        )\n",
    "        video_frames_for_vis.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start-session-title",
   "metadata": {},
   "source": [
    "### 推論セッションの開始 (Opening an inference session on this video)\n",
    "\n",
    "SAM 3はインタラクティブなビデオセグメンテーションのためにステートフルな推論を必要とするため、このビデオに対する **推論セッション** を初期化する必要があります。\n",
    "\n",
    "初期化中に、すべてのビデオフレームを読み込み、そのピクセルをセッション状態に保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"start_session\",\n",
    "        resource_path=video_path,\n",
    "    )\n",
    ")\n",
    "session_id = response[\"session_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-prompt-video-title",
   "metadata": {},
   "source": [
    "### テキストによるビデオ概念セグメンテーション (Video promptable concept segmentation with text)\n",
    "\n",
    "SAM 3を使用すると、自然言語でオブジェクトを記述でき、モデルはビデオ全体を通してそのオブジェクトのすべてのインスタンスを自動的に検出して追跡します。\n",
    "\n",
    "以下の例では、フレーム0にテキストプロンプトを追加し、ビデオ全体に伝播させます。ここでは、ビデオ内のすべての人を検出するために \"person\"（人）というテキストプロンプトを使用します。SAM 3は自動的に複数の人物インスタンスを識別し、それぞれに一意のオブジェクトIDを割り当てます。\n",
    "\n",
    "バッファの設定のため、最初の呼び出しは遅くなる可能性があることに注意してください。**速度を測定する場合は、以下のすべてのセルを再実行できます。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reset-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：すでに1つのテキストプロンプトを実行していて、別のテキストプロンプトに切り替えたい場合、\n",
    "# まずセッションをリセットする必要があります（そうしないと結果が間違ったものになります）\n",
    "_ = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"reset_session\",\n",
    "        session_id=session_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-text-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text_str = \"person\"\n",
    "frame_idx = 0  # フレーム0にテキストプロンプトを追加\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        text=prompt_text_str,\n",
    "    )\n",
    ")\n",
    "out = response[\"outputs\"]\n",
    "\n",
    "plt.close(\"all\")\n",
    "visualize_formatted_frame_output(\n",
    "    frame_idx,\n",
    "    video_frames_for_vis,\n",
    "    outputs_list=[prepare_masks_for_visualization({frame_idx: out})],\n",
    "    titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "    figsize=(6, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "propagate-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレーム0からビデオの終わりまで出力を伝播し、すべての出力を収集します\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "\n",
    "# 最後に、可視化のために出力を再フォーマットし、60フレームごとに出力をプロットします\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remove-object-title",
   "metadata": {},
   "source": [
    "### オブジェクトの削除 (Removing objects)\n",
    "\n",
    "IDを使用して個々のオブジェクトを削除できます。\n",
    "\n",
    "例として、オブジェクト2（手前のダンサー）を削除してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remove-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手前のダンサーであるID 2を選択します\n",
    "obj_id = 2\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"remove_object\",\n",
    "        session_id=session_id,\n",
    "        obj_id=obj_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "propagate-after-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレーム0からビデオの終わりまで出力を伝播し、すべての出力を収集します\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "\n",
    "# 最後に、可視化のために出力を再フォーマットし、60フレームごとに出力をプロットします\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add-object-points-title",
   "metadata": {},
   "source": [
    "### ポイントプロンプトによる新しいオブジェクトの追加 (Adding new objects with point prompts)\n",
    "\n",
    "ポイントプロンプトを使用して新しいオブジェクトを追加できます。\n",
    "\n",
    "気が変わって、手前のダンサー（先ほど削除した人物）を戻したいとします。インタラクティブなクリックを使用して彼女を戻すことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-img-dims",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = Image.fromarray(load_frame(video_frames_for_vis[0]))\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = sample_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-point-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ポイントプロンプトでダンサーを戻しましょう。\n",
    "# 1回のポジティブクリックを使用してダンサーを戻します。\n",
    "\n",
    "frame_idx = 0\n",
    "obj_id = 2\n",
    "points_abs = np.array(\n",
    "    [\n",
    "        [760, 550],  # ポジティブクリック\n",
    "    ]\n",
    ")\n",
    "# ポジティブクリックはラベル1、ネガティブクリックはラベル0です\n",
    "labels = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-point-add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ポイントとラベルをテンソルに変換し、相対座標にも変換します\n",
    "points_tensor = torch.tensor(\n",
    "    abs_to_rel_coords(points_abs, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"),\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        points=points_tensor,\n",
    "        point_labels=points_labels_tensor,\n",
    "        obj_id=obj_id,\n",
    "    )\n",
    ")\n",
    "out = response[\"outputs\"]\n",
    "\n",
    "plt.close(\"all\")\n",
    "visualize_formatted_frame_output(\n",
    "    frame_idx,\n",
    "    video_frames_for_vis,\n",
    "    outputs_list=[prepare_masks_for_visualization({frame_idx: out})],\n",
    "    titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "    figsize=(6, 4),\n",
    "    points_list=[points_abs],\n",
    "    points_labels_list=[labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "propagate-after-add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレーム0からビデオの終わりまで出力を伝播し、すべての出力を収集します\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "\n",
    "# 最後に、可視化のために出力を再フォーマットし、60フレームごとに出力をプロットします\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refine-object-title",
   "metadata": {},
   "source": [
    "### ポイントプロンプトによる既存オブジェクトの修正 (Refining an existing object with point prompts)\n",
    "\n",
    "ポイントプロンプトを使用して、既存のオブジェクトのセグメンテーションマスクを修正することもできます。\n",
    "\n",
    "（また）気が変わったと仮定して、オブジェクトID 2（先ほど戻した手前のダンサー）について、全身ではなくTシャツだけをセグメントしたいとします。いくつかのポジティブクリックとネガティブクリックでセグメンテーションマスクを調整できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refine-clicks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手前のダンサーについて、全身ではなくTシャツだけをセグメントしたいとします\n",
    "# 2つのポジティブクリックと2つのネガティブクリックを使用してシャツを選択します\n",
    "\n",
    "frame_idx = 0\n",
    "obj_id = 2\n",
    "points_abs = np.array(\n",
    "    [\n",
    "        [740, 450],  # ポジティブクリック\n",
    "        [760, 630],  # ネガティブクリック\n",
    "        [840, 640],  # ネガティブクリック\n",
    "        [760, 550],  # ポジティブクリック\n",
    "    ]\n",
    ")\n",
    "# ポジティブクリックはラベル1、ネガティブクリックはラベル0です\n",
    "labels = np.array([1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-refine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ポイントとラベルをテンソルに変換し、相対座標にも変換します\n",
    "points_tensor = torch.tensor(\n",
    "    abs_to_rel_coords(points_abs, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"),\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        points=points_tensor,\n",
    "        point_labels=points_labels_tensor,\n",
    "        obj_id=obj_id,\n",
    "    )\n",
    ")\n",
    "out = response[\"outputs\"]\n",
    "\n",
    "plt.close(\"all\")\n",
    "visualize_formatted_frame_output(\n",
    "    frame_idx,\n",
    "    video_frames_for_vis,\n",
    "    outputs_list=[prepare_masks_for_visualization({frame_idx: out})],\n",
    "    titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "    figsize=(6, 4),\n",
    "    points_list=[points_abs],\n",
    "    points_labels_list=[labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "propagate-after-refine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フレーム0からビデオの終わりまで出力を伝播し、すべての出力を収集します\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id)\n",
    "\n",
    "# 最後に、可視化のために出力を再フォーマットし、60フレームごとに出力をプロットします\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-session-title",
   "metadata": {},
   "source": [
    "### セッションを閉じる (Close session)\n",
    "\n",
    "各セッションは単一のビデオに紐付いています。推論後にセッションを閉じてリソースを解放できます。\n",
    "\n",
    "（その後、別のビデオで新しいセッションを開始できます。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後に、推論セッションを閉じてGPUリソースを解放します\n",
    "# （別のビデオで新しいセッションを開始できます）\n",
    "_ = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"close_session\",\n",
    "        session_id=session_id,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-title",
   "metadata": {},
   "source": [
    "### クリーンアップ (Clean-up)\n",
    "\n",
    "すべての推論が終了したら、predictorをシャットダウンしてマルチGPUプロセスグループを解放できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shutdown-predictor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべての推論が終了した後、predictorをシャットダウンして\n",
    "# マルチGPUプロセスグループを解放できます\n",
    "predictor.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}