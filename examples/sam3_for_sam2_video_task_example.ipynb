{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# SAM 3 によるビデオオブジェクトセグメンテーション (Video object segmentation with SAM 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-setup-md",
   "metadata": {},
   "source": [
    "## Google Colab セットアップ\n",
    "\n",
    "Google Colabで実行する場合は、以下のセルを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabで実行しているかチェック\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Google Colab環境で実行中\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"ローカル環境で実行中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabの場合、必要なパッケージをインストール\n",
    "if IN_COLAB:\n",
    "    !pip install -q pycocotools\n",
    "    # SAM3のインストール (GitHubリポジトリから)\n",
    "    !pip install -q git+https://github.com/facebookresearch/sam3.git\n",
    "    print(\"パッケージのインストールが完了しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveをマウント（データをドライブに保存している場合）\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Driveがマウントされました\")\n",
    "    print(\"データのパスは /content/drive/MyDrive/... のように指定してください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-upload-md",
   "metadata": {},
   "source": [
    "### データのアップロード方法\n",
    "\n",
    "Google Colabでデータを使用する方法は3つあります:\n",
    "\n",
    "1. **Google Driveを使用**: 上のセルでマウント後、`/content/drive/MyDrive/` 以下のパスを指定\n",
    "2. **ファイルを直接アップロード**: 左側のファイルアイコンから手動アップロード\n",
    "3. **Colabのファイルアップロードウィジェットを使用**: 以下のコードを実行\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # ファイル選択ダイアログが表示されます\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d0b3c-4207-442d-969c-aa1cbb8fd4ad",
   "metadata": {},
   "source": [
    "このノートブックでは、`Sam3TrackerPredictor` クラスを使用して、SAM 3 をビデオオブジェクトセグメンテーションに使用する方法を示します。\n",
    "\n",
    "このノートブックは、インタラクティブなビデオセグメンテーションのための SAM 2 API に従っています。\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam3/blob/main/notebooks/sam3_for_sam2_video_task_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26616201-06df-435b-98fd-ad17c373bb4a",
   "metadata": {},
   "source": [
    "## 環境設定 (Environment Set-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf",
   "metadata": {},
   "source": [
    "まず、リポジトリの [インストール手順](https://github.com/facebookresearch/sam3?tab=readme-ov-file#installation) に従って、環境に `sam3` をインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib scikit-learn\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam3.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
   "metadata": {},
   "source": [
    "## セットアップ (Set-up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cae821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 計算に使用するデバイスを選択\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # ノートブック全体でbfloat16を使用\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # Ampere GPU向けにtfloat32を有効化 (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 3 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sam3\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sam3.visualization_utils import show_box, show_mask, show_points\n",
    "\n",
    "# 軸タイトルのフォントサイズ\n",
    "plt.rcParams[\"axes.titlesize\"] = 12\n",
    "plt.rcParams[\"figure.titlesize\"] = 12\n",
    "\n",
    "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### SAM 3 追跡予測器の読み込み (Loading the SAM 3 tracking predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam3.model_builder import build_sam3_video_model\n",
    "\n",
    "sam3_model = build_sam3_video_model()\n",
    "predictor = sam3_model.tracker\n",
    "predictor.backbone = sam3_model.detector.backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {},
   "source": [
    "#### 推論状態の初期化 (Initialize the inference state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {},
   "source": [
    "SAM 2 と同様に、SAM 3 もインタラクティブなビデオセグメンテーションのためにステートフルな推論を必要とするため、このビデオ上で **推論状態 (inference state)** を初期化する必要があります。\n",
    "\n",
    "初期化中に、`video_path` 内のすべての JPEG フレームを読み込み、それらのピクセルを `inference_state` に保存します (以下のプログレスバーで表示されます)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = f\"{sam3_root}/assets/videos/bedroom.mp4\"\n",
    "inference_state = predictor.init_state(video_path=video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1f3f6-d74d-4016-934c-8d2a14d1a543",
   "metadata": {},
   "source": [
    "### 例 1: 1つのオブジェクトのセグメンテーションと追跡 (Example 1: Segment & track one object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {},
   "source": [
    "注: この `inference_state` を使用して以前に追跡を実行したことがある場合は、まず `clear_all_points_in_video` を介してリセットしてください。\n",
    "\n",
    "(以下のセルは説明用であり、この `inference_state` は上で新しく初期化されたばかりなので、ここで `clear_all_points_in_video` を呼び出す必要はありません。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.clear_all_points_in_video(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    "#### ステップ 1: フレームに最初のクリックを追加する (Step 1: Add a first click on a frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c7749-b523-4691-aad0-7558c5d1d68c",
   "metadata": {},
   "source": [
    "手始めに、左側の子供をセグメント化してみましょう。\n",
    "\n",
    "ここでは、(x, y) = (210, 350) にラベル `1` で **ポジティブクリック** を行います。座標とラベルを `add_new_points` API に送信します。\n",
    "\n",
    "注: ラベル `1` は *ポジティブクリック (領域を追加)* を示し、ラベル `0` は *ネガティブクリック (領域を削除)* を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6778a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覚化のためにフレームを読み込む\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "video_frames_for_vis = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    video_frames_for_vis.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "cap.release()\n",
    "frame0 = video_frames_for_vis[0]\n",
    "\n",
    "width, height = frame0.shape[1], frame0.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e749bab-0f36-4173-bf8d-0c20cd5214b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # インタラクトするフレームインデックス\n",
    "ann_obj_id = 1  # インタラクトする各オブジェクトに一意のIDを与える (任意の整数で可)\n",
    "\n",
    "# 手始めに (x, y) = (210, 350) にポジティブクリックを追加\n",
    "points = np.array([[210, 350]], dtype=np.float32)\n",
    "# ラベルについて、`1` はポジティブクリック、`0` はネガティブクリックを意味する\n",
    "labels = np.array([1], np.int32)\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks = predictor.add_new_points(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    "    clear_old_points=False,\n",
    ")\n",
    "\n",
    "# 現在の (インタラクトした) フレーム上の結果を表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(frame0)\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((video_res_masks[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89457875-93fa-40ed-b6dc-4e1c971a27f9",
   "metadata": {},
   "source": [
    "#### ステップ 2: 2回目のクリックを追加して予測を修正する (Step 2: Add a second click to refine the prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75eb21b-1413-452c-827b-a04093c30c78",
   "metadata": {},
   "source": [
    "左側の子供をセグメント化したかったのですが、モデルはショートパンツのマスクのみを予測しているようです。これは、ターゲットオブジェクトが何であるかについて、1回のクリックでは曖昧さが生じるために起こり得ます。子供のシャツにもう一度ポジティブクリックを行うことで、このフレーム上のマスクを修正できます。\n",
    "\n",
    "ここでは、マスクを拡張するために、(x, y) = (250, 220) にラベル `1` で **2回目のポジティブクリック** を行います。\n",
    "\n",
    "注: `add_new_points` を呼び出すときは、(最後のクリックだけでなく) **すべてのクリックとそのラベル** を送信する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab3ec7-2537-4158-bf98-3d0977d8908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # インタラクトするフレームインデックス\n",
    "ann_obj_id = 1  # インタラクトする各オブジェクトに一意のIDを与える (任意の整数で可)\n",
    "\n",
    "# マスクを修正するために (x, y) = (250, 220) に2回目のポジティブクリックを追加\n",
    "# すべてのクリック (とそのラベル) を `add_new_points_or_box` に送信する\n",
    "points = np.array([[210, 350], [250, 220]], dtype=np.float32)\n",
    "# ラベルについて、`1` はポジティブクリック、`0` はネガティブクリックを意味する\n",
    "labels = np.array([1, 1], np.int32)\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    "    clear_old_points=False,\n",
    ")\n",
    "\n",
    "# 現在の (インタラクトした) フレーム上の結果を表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(frame0)\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((video_res_masks[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ab457-d91d-4ac8-b350-fbcd549fd3fd",
   "metadata": {},
   "source": [
    "この2回目の修正クリックにより、フレーム0上の子供全体のセグメンテーションマスクが得られました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52015ac-1b7b-4c59-bca3-c2b28484cf46",
   "metadata": {},
   "source": [
    "#### ステップ 3: プロンプトを伝播させてビデオ全体でマスクレットを取得する (Step 3: Propagate the prompts to get the masklet across the video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b025bd-cd58-4bfb-9572-c8d2fd0a02ef",
   "metadata": {},
   "source": [
    "ビデオ全体を通してマスクレットを取得するために、`propagate_in_video` API を使用してプロンプトを伝播させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e932-b0d5-4983-9718-6ee77d1ac31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ビデオ全体で伝播を実行し、結果を辞書に収集する\n",
    "video_segments = {}  # video_segments はフレームごとのセグメンテーション結果を含む\n",
    "for frame_idx, obj_ids, low_res_masks, video_res_masks, obj_scores in predictor.propagate_in_video(inference_state, start_frame_idx=0, max_frame_num_to_track=240, reverse=False, propagate_preflight=True):\n",
    "    video_segments[frame_idx] = {\n",
    "        out_obj_id: (video_res_masks[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# 数フレームごとにセグメンテーション結果を描画\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_frames_for_vis), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(video_frames_for_vis[out_frame_idx])\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e801b70-72df-4a72-b3fe-84f145e5e3f6",
   "metadata": {},
   "source": [
    "#### ステップ 4: 新しいプロンプトを追加してマスクレットをさらに修正する (Step 4: Add new prompts to further refine the masklet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478958ab-29b4-4a75-bba4-adb1b03d0a2b",
   "metadata": {},
   "source": [
    "上記の出力マスクレットを見ると、フレーム150の境界の詳細にいくつかの小さな欠陥があるようです。\n",
    "\n",
    "SAM 3 を使用すると、モデルの予測をインタラクティブに修正できます。このフレームの (x, y) = (82, 415) にラベル `0` で **ネガティブクリック** を追加して、マスクレットを修正できます。ここでは、修正したいフレームインデックスを示すために、異なる `frame_idx` 引数を使用して `add_new_points_or_box` API を呼び出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a572ea9-5b7e-479c-b30c-93c38b121131",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 150  # このフレームの詳細をさらに修正する\n",
    "ann_obj_id = 1  # インタラクトするオブジェクトに一意のIDを与える (任意の整数で可)\n",
    "\n",
    "# さらなる修正前のセグメントを表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx} -- before refinement\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "show_mask(video_segments[ann_frame_idx][ann_obj_id], plt.gca(), obj_id=ann_obj_id)\n",
    "\n",
    "# セグメントを修正するために、このフレームの (x, y) = (82, 415) にネガティブクリックを追加\n",
    "points = np.array([[82, 410]], dtype=np.float32)\n",
    "# ラベルについて、`1` はポジティブクリック、`0` はネガティブクリックを意味する\n",
    "labels = np.array([0], np.int32)\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    "    clear_old_points=False,\n",
    ")\n",
    "\n",
    "\n",
    "# さらなる修正後のセグメントを表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx} -- after refinement\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((video_res_masks > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3950a-acf1-435c-bd64-94297267b5e9",
   "metadata": {},
   "source": [
    "#### ステップ 5: プロンプトを (再度) 伝播させてビデオ全体でマスクレットを取得する (Step 5: Propagate the prompts (again) to get the masklet across the video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1954ecf-c2ec-4f9c-8d10-c4f527a10cd2",
   "metadata": {},
   "source": [
    "ビデオ全体の更新されたマスクレットを取得しましょう。ここでは、上記の新しい修正クリックを追加した後、すべてのプロンプトを伝播させるために、再度 `propagate_in_video` を呼び出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa96690-4a38-4a24-aa17-fd2f4db0e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ビデオ全体で伝播を実行し、結果を辞書に収集する\n",
    "video_segments = {}  # video_segments はフレームごとのセグメンテーション結果を含む\n",
    "for frame_idx, obj_ids, low_res_masks, video_res_masks, obj_scores in predictor.propagate_in_video(inference_state, start_frame_idx=0, max_frame_num_to_track=300, reverse=False, propagate_preflight=True):\n",
    "    video_segments[frame_idx] = {\n",
    "        out_obj_id: (video_res_masks[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# 数フレームごとにセグメンテーション結果を描画\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_frames_for_vis), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(video_frames_for_vis[out_frame_idx])\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607507e3-6a2b-4fd7-944c-2371bdab9d01",
   "metadata": {},
   "source": [
    "セグメントはすべてのフレームで良好に見えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502bb5a-3e1f-43d0-9f58-33f8676fff0d",
   "metadata": {},
   "source": [
    "### 例 2: ボックスプロンプトを使用してオブジェクトをセグメント化する (Example 2: Segment an object using box prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d26c8-0432-48c6-997e-4a3b77bb5f6d",
   "metadata": {},
   "source": [
    "注: この `inference_state` を使用して以前に追跡を実行したことがある場合は、まず `clear_all_points_in_video` を介してリセットしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe9183-abbb-4283-b0cb-d24f3d7beb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.clear_all_points_in_video(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6eae9-0f4c-434f-8089-a46c9ca59da5",
   "metadata": {},
   "source": [
    "クリックを入力として使用することに加えて、SAM 3 は **バウンディングボックス** を介したビデオ内のオブジェクトのセグメンテーションと追跡もサポートしています。\n",
    "\n",
    "以下の例では、`add_new_points_or_box` API への入力として、フレーム0上の (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) の **ボックスプロンプト** を使用して、右側の子供をセグメント化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfb273-4e14-495b-bd89-87a8baf52ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # インタラクトするフレームインデックス\n",
    "ann_obj_id = 4  # インタラクトする各オブジェクトに一意のIDを与える (任意の整数で可)\n",
    "\n",
    "# 手始めに (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) にボックスを追加\n",
    "box = np.array([[300, 0, 500, 400]], dtype=np.float32)\n",
    "\n",
    "rel_box = [[xmin / width, ymin / height, xmax / width, ymax / height] for xmin, ymin, xmax, ymax in box]\n",
    "rel_box = np.array(rel_box, dtype=np.float32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    box=rel_box,\n",
    ")\n",
    "\n",
    "# 現在の (インタラクトした) フレーム上の結果を表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "show_box(box[0], plt.gca())\n",
    "show_mask((video_res_masks[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f9ba7-bf4d-47e5-9b02-8a424cab42cc",
   "metadata": {},
   "source": [
    "ここでは、入力バウンディングボックスがオブジェクトの周りに完全に密着していなくても、SAM 3 は子供全体のかなり良好なセグメンテーションマスクを取得しています。\n",
    "\n",
    "前の例と同様に、ボックスプロンプトを使用したときに返されたマスクが完璧でない場合は、ポジティブクリックまたはネガティブクリックを使用して出力をさらに **修正** することもできます。これを説明するために、ここでは (x, y) = (460, 60) にラベル `1` で **ポジティブクリック** を行い、子供の髪の周りのセグメントを拡張します。\n",
    "\n",
    "注: ボックスプロンプトからのセグメンテーションマスクを修正するには、`add_new_points_or_box` を呼び出すときに、**元のボックス入力とその後のすべての修正クリックとそのラベルの両方** を送信する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54906315-ab4c-4088-b866-4c22134d5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # インタラクトするフレームインデックス\n",
    "ann_obj_id = 4  # インタラクトする各オブジェクトに一意のIDを与える (任意の整数で可)\n",
    "\n",
    "# マスクを修正するために (x, y) = (460, 60) にポジティブクリックを追加\n",
    "points = np.array([[460, 60]], dtype=np.float32)\n",
    "# ラベルについて、`1` はポジティブクリック、`0` はネガティブクリックを意味する\n",
    "labels = np.array([1], np.int32)\n",
    "# また、新しい修正クリックと一緒に元のボックス入力も\n",
    "# `add_new_points_or_box` に送信する必要があることに注意してください\n",
    "box = np.array([[300, 0, 500, 400]], dtype=np.float32)\n",
    "\n",
    "rel_box = [[xmin / width, ymin / height, xmax / width, ymax / height] for xmin, ymin, xmax, ymax in box]\n",
    "rel_box = np.array(rel_box, dtype=np.float32)\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    "    box=rel_box,\n",
    ")\n",
    "\n",
    "# 現在の (インタラクトした) フレーム上の結果を表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "show_box(box[0], plt.gca())\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((video_res_masks[0][0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73128cd6-dbfa-49f7-8d79-1a8e19835f7f",
   "metadata": {},
   "source": [
    "その後、ビデオ全体を通してマスクレットを取得するために、`propagate_in_video` API を使用してプロンプトを伝播させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd90557-a0dc-442e-b091-9c74c831bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ビデオ全体で伝播を実行し、結果を辞書に収集する\n",
    "video_segments = {}  # video_segments はフレームごとのセグメンテーション結果を含む\n",
    "for frame_idx, obj_ids, low_res_masks, video_res_masks, obj_scores in predictor.propagate_in_video(inference_state, start_frame_idx=0, max_frame_num_to_track=300, reverse=False, propagate_preflight=True):\n",
    "    video_segments[frame_idx] = {\n",
    "        out_obj_id: (video_res_masks[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# 数フレームごとにセグメンテーション結果を描画\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_frames_for_vis), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(video_frames_for_vis[out_frame_idx])\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023f91f-0cc5-4980-ae8e-a13c5749112b",
   "metadata": {},
   "source": [
    "クリックやボックスに加えて、SAM 3 は `Sam3TrackerPredictor` クラスの `add_new_mask` メソッドを介して、**マスクプロンプト** を入力として直接使用することもサポートしていることに注意してください。これは、例えば半教師ありVOS評価などで役立ちます (例として [tools/vos_inference.py](https://github.com/facebookresearch/sam2/blob/main/tools/vos_inference.py) を参照してください)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68",
   "metadata": {},
   "source": [
    "### 例 3: 複数のオブジェクトを同時にセグメント化する (Example 3: Segment multiple objects simultaneously)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6c04c-3072-4876-b394-879321a48c4a",
   "metadata": {},
   "source": [
    "注: この `inference_state` を使用して以前に追跡を実行したことがある場合は、まず `clear_all_points_in_video` を介してリセットしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b874c8-9f39-42d3-a667-54a0bd696410",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.clear_all_points_in_video(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3f7e6-4821-468c-84e4-f3a0435c9149",
   "metadata": {},
   "source": [
    "#### ステップ 1: フレームに2つのオブジェクトを追加する (Step 1: Add two objects on a frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95158714-86d7-48a9-8365-b213f97cc9ca",
   "metadata": {},
   "source": [
    "SAM 3 は、2つ以上のオブジェクトを同時にセグメント化して追跡することもできます。もちろん、1つずつ行うこともできますが、バッチ処理する方が効率的です (例えば、計算コストを削減するためにオブジェクト間で画像特徴を共有できます)。\n",
    "\n",
    "今回は、オブジェクトの部分に焦点を当て、このビデオ内の **両方の子供のシャツ** をセグメント化してみましょう。ここでは、これら2つのオブジェクトのプロンプトを追加し、それぞれに一意のオブジェクトIDを割り当てます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d896d-3cd5-4fa0-9230-f33e217035dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {}  # 視覚化のために追加したすべてのクリックを保持する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9ac57-b14a-4237-828d-927e422c518b",
   "metadata": {},
   "source": [
    "フレーム0の (x, y) = (200, 300) に **ポジティブクリック** で最初のオブジェクト (左の子供のシャツ) を追加します。\n",
    "\n",
    "オブジェクトID `2` を割り当てます (任意の整数で構いませんが、追跡する各オブジェクトに対して一意である必要があります)。これは、クリックしているオブジェクトを区別するために `add_new_points_or_box` API に渡されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13432fc-f467-44d8-adfe-3e0c488046b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # インタラクトするフレームインデックス\n",
    "ann_obj_id = 2  # インタラクトする各オブジェクトに一意のIDを与える (任意の整数で可)\n",
    "\n",
    "# 最初のオブジェクトを開始するために (x, y) = (200, 300) にポジティブクリックを追加\n",
    "points = np.array([[200, 300]], dtype=np.float32)\n",
    "# ラベルについて、`1` はポジティブクリック、`0` はネガティブクリックを意味する\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    ")\n",
    "\n",
    "\n",
    "# 現在の (インタラクトした) フレーム上の結果を表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(points, labels, plt.gca())\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((video_res_masks[i][0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbd51b-e1e2-4c36-99ec-1d9a1b49b0cd",
   "metadata": {},
   "source": [
    "うーん、今回は子供のシャツだけを選択したかったのですが、モデルは子供全体のマスクを予測しています。(x, y) = (275, 175) に **ネガティブクリック** を行って予測を修正しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecf61d-662b-4f98-ae62-46557b219842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初のオブジェクトを追加\n",
    "ann_frame_idx = 0  # インタラクトするフレームインデックス\n",
    "ann_obj_id = 2  # インタラクトする各オブジェクトに一意のIDを与える (任意の整数で可)\n",
    "\n",
    "# 最初のオブジェクトを修正するために (x, y) = (275, 175) に2回目のネガティブクリックを追加\n",
    "# すべてのクリック (とそのラベル) を `add_new_points_or_box` に送信する\n",
    "points = np.array([[200, 300], [275, 175]], dtype=np.float32)\n",
    "# ラベルについて、`1` はポジティブクリック、`0` はネガティブクリックを意味する\n",
    "labels = np.array([1, 0], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=rel_points,\n",
    "    labels=points_labels_tensor,\n",
    ")\n",
    "\n",
    "# 現在の (インタラクトした) フレーム上の結果を表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(points, labels, plt.gca())\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((video_res_masks[i][0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194718c1-734d-446c-a3ef-361057de2f31",
   "metadata": {},
   "source": [
    "2回目のネガティブクリックの後、最初のオブジェクトとして左側の子供のシャツを取得できました。\n",
    "\n",
    "フレーム0の (x, y) = (400, 150) にポジティブクリックを行い、2番目のオブジェクト (右側の子供のシャツ) に進みましょう。ここでは、この2番目のオブジェクトにオブジェクトID `3` を割り当てます (任意の整数で構いませんが、追跡する各オブジェクトに対して一意である必要があります)。\n",
    "\n",
    "注: 複数のオブジェクトがある場合、`add_new_points_or_box` API は各オブジェクトのマスクのリストを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca1bde-62a4-40e6-98e4-15606441e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # インタラクトするフレームインデックス\n",
    "ann_obj_id = 3  # インタラクトする各オブジェクトに一意のIDを与える (任意の整数で可)\n",
    "\n",
    "# 追跡したい2番目のオブジェクトに進みます (オブジェクトID `3` を付与)\n",
    "# (x, y) = (400, 150) にポジティブクリック\n",
    "points = np.array([[400, 150]], dtype=np.float32)\n",
    "# ラベルについて、`1` はポジティブクリック、`0` はネガティブクリックを意味する\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "\n",
    "# `add_new_points_or_box` は、このインタラクトしたフレーム上でこれまでに追加されたすべてのオブジェクトのマスクを返します\n",
    "_, out_obj_ids, low_res_masks, video_res_masks = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    ")\n",
    "\n",
    "# 現在の (インタラクトした) フレーム上のすべてのオブジェクトの結果を表示\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(points, labels, plt.gca())\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((video_res_masks[i][0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7add8-d577-4597-ae2f-654b8c7b05e0",
   "metadata": {},
   "source": [
    "今回は、モデルはたった1回のクリックで追跡したいシャツのマスクを予測しました。素晴らしい！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448733b8-ea8b-4078-995f-b676c3b558ba",
   "metadata": {},
   "source": [
    "#### ステップ 2: プロンプトを伝播させてビデオ全体でマスクレットを取得する (Step 2: Propagate the prompts to get masklets across the video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd73de-d669-41c8-b6ba-943883f0caa2",
   "metadata": {},
   "source": [
    "これで、両方のオブジェクトのプロンプトを伝播させて、ビデオ全体を通してそれらのマスクレットを取得します。\n",
    "\n",
    "注: 複数のオブジェクトがある場合、`propagate_in_video` API は各オブジェクトのマスクのリストを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ビデオ全体で伝播を実行し、結果を辞書に収集する\n",
    "video_segments = {}  # video_segments はフレームごとのセグメンテーション結果を含む\n",
    "for frame_idx, obj_ids, low_res_masks, video_res_masks, obj_scores in predictor.propagate_in_video(inference_state, start_frame_idx=0, max_frame_num_to_track=300, reverse=False, propagate_preflight=True):\n",
    "    video_segments[frame_idx] = {\n",
    "        out_obj_id: (video_res_masks[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# 数フレームごとにセグメンテーション結果を描画\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_frames_for_vis), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(video_frames_for_vis[out_frame_idx])\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0b9d7-c78f-432b-afb0-11f2ea5b652a",
   "metadata": {},
   "source": [
    "このビデオでは、両方の子供のシャツがうまくセグメント化されているようです。\n",
    "\n",
    "さあ、あなた自身のビデオやユースケースで SAM 3 を試してみてください！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}